{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import os\n",
    "import matplotlib.image as mpimg\n",
    "import cv2\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "TRAIN_PATH = \"/datasets/tmp/aayang/tiny-imagenet-200/train/\"\n",
    "VAL_IMAGES_PATH = \"/datasets/tmp/aayang/tiny-imagenet-200/val/images/\"\n",
    "VAL_ANNO_FILE = \"/datasets/tmp/aayang/tiny-imagenet-200/val/val_annotations.txt\"\n",
    "TRAINING_IMAGE_SIZE = 64\n",
    "\n",
    "LOAD_BATCH_SIZE = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_images(start=0, batch_size=200):\n",
    "    images = [] #np.ndarray(shape=(batch_size*200, 64,64,3))\n",
    "    labels = []\n",
    "    file_names = []\n",
    "    class_num = 0\n",
    "    \n",
    "    for class_folder in os.listdir(TRAIN_PATH):\n",
    "        images_folder = TRAIN_PATH + class_folder + \"/up_tint/\"\n",
    "        x = 0\n",
    "        \n",
    "        for image_name in os.listdir(images_folder):\n",
    "            if x < start:\n",
    "                x += 1\n",
    "                continue\n",
    "            \n",
    "            image_path = os.path.join(images_folder, image_name)\n",
    "            image = mpimg.imread(image_path)\n",
    "            \n",
    "            if image.shape == (TRAINING_IMAGE_SIZE, TRAINING_IMAGE_SIZE, 3):\n",
    "                images.append(image/256.0)\n",
    "                labels.append(class_folder)\n",
    "                file_names.append(image_name)\n",
    "    \n",
    "            x += 1      \n",
    "            #if x%100 == 0: \n",
    "            #    print \"class\", class_num, \"-\", x\n",
    "                \n",
    "            if x > start+batch_size:\n",
    "                break\n",
    "        class_num += 1\n",
    "                        \n",
    "    return (np.array(images), np.array(labels), np.array(file_names))\n",
    "\n",
    "def get_one_hot(le, labels):\n",
    "    labels_encoded = le.transform(labels)\n",
    "    y = keras.utils.to_categorical(labels_encoded)\n",
    "    return y\n",
    "    \n",
    "def randomize(images, labels, files):\n",
    "    shuffle_index = np.random.permutation(len(images))\n",
    "    \n",
    "    shuffled_images = images[shuffle_index]\n",
    "    shuffled_labels = labels[shuffle_index]\n",
    "    shuffled_files  = files[shuffle_index]\n",
    "    \n",
    "    return (shuffled_images, shuffled_labels, shuffled_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_validation_images():\n",
    "    with open(VAL_ANNO_FILE) as f:\n",
    "        lines = f.read().splitlines()\n",
    "        \n",
    "    labels = []\n",
    "    images = []\n",
    "    idx = 0\n",
    "    \n",
    "    for line in lines:\n",
    "        image_att = line.split(\"\\t\")\n",
    "        image_name = image_att[0]\n",
    "        image_label = image_att[1]\n",
    "        \n",
    "        image_path = os.path.join(VAL_IMAGES_PATH, image_name)\n",
    "        image = mpimg.imread(image_path)\n",
    "        \n",
    "        if image.shape == (TRAINING_IMAGE_SIZE, TRAINING_IMAGE_SIZE, 3):\n",
    "            images.append(image/256.0)\n",
    "            labels.append(image_label)\n",
    "            \n",
    "    return (np.array(images), np.array(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Layer CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "shallow = Sequential()\n",
    "# input: 100x100 images with 3 channels -> (100, 100, 3) tensors.\n",
    "# this applies 32 convolution filters of size 3x3 each.\n",
    "shallow.add(Conv2D(64, (3, 3), activation='relu', input_shape=(64,64,3)))\n",
    "shallow.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#shallow.add(Conv2D(128, (3, 3), activation='relu', padding=\"same\"))\n",
    "#shallow.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#shallow.add(Conv2D(256, (5, 5), activation='relu', padding=\"same\"))\n",
    "#shallow.add(Conv2D(256, (5, 5), activation='relu', padding=\"same\"))\n",
    "#shallow.add(Conv2D(256, (5, 5), activation='relu', padding=\"same\"))\n",
    "#shallow.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "shallow.add(Flatten())\n",
    "shallow.add(Dense(1024, activation='relu'))\n",
    "#shallow.add(Dense(1024, activation='relu'))\n",
    "#shallow.add(Dropout(0.5))\n",
    "#shallow.add(Dense(256, activation='relu'))\n",
    "shallow.add(Dense(200, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "shallow.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=[\"accuracy\"])\n",
    "shallow.load_weights(\"shallow_weights_augment_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Layer CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "six_layer = Sequential()\n",
    "\n",
    "six_layer.add(Conv2D(64, (3, 3), activation='relu', input_shape=(64,64,3)))\n",
    "six_layer.add(Conv2D(64, (3, 3), activation='relu', padding=\"same\"))\n",
    "six_layer.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "six_layer.add(Conv2D(128, (3, 3), activation='relu', padding=\"same\"))\n",
    "six_layer.add(Conv2D(128, (3, 3), activation='relu', padding=\"same\"))\n",
    "six_layer.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "six_layer.add(Dropout(0.25))\n",
    "\n",
    "#six_layer.add(Conv2D(256, (5, 5), activation='relu', padding=\"same\"))\n",
    "#six_layer.add(Conv2D(256, (5, 5), activation='relu', padding=\"same\"))\n",
    "#six_layer.add(Conv2D(256, (5, 5), activation='relu', padding=\"same\"))\n",
    "#six_layer.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "six_layer.add(Flatten())\n",
    "six_layer.add(Dense(1024, activation='relu'))\n",
    "#six_layer.add(Dense(1024, activation='relu'))\n",
    "#six_layer.add(Dropout(0.5))\n",
    "#six_layer.add(Dense(256, activation='relu'))\n",
    "six_layer.add(Dense(200, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "six_layer.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=[\"accuracy\"])\n",
    "six_layer.load_weights(\"six_weights_augment_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9 Layer CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nine_layer = Sequential()\n",
    "# input: 100x100 images with 3 channels -> (100, 100, 3) tensors.\n",
    "# this applies 32 convolution filters of size 3x3 each.\n",
    "nine_layer.add(Conv2D(64, (3, 3), activation='relu', input_shape=(64,64,3)))\n",
    "nine_layer.add(Conv2D(64, (3, 3), activation='relu', padding=\"same\"))\n",
    "nine_layer.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "nine_layer.add(Conv2D(128, (3, 3), activation='relu', padding=\"same\"))\n",
    "nine_layer.add(Conv2D(128, (3, 3), activation='relu', padding=\"same\"))\n",
    "nine_layer.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "nine_layer.add(Dropout(0.25))\n",
    "\n",
    "nine_layer.add(Conv2D(256, (3, 3), activation='relu', padding=\"same\"))\n",
    "nine_layer.add(Conv2D(256, (3, 3), activation='relu', padding=\"same\"))\n",
    "nine_layer.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "nine_layer.add(Flatten())\n",
    "nine_layer.add(Dense(1024, activation='relu'))\n",
    "nine_layer.add(Dense(1024, activation='relu'))\n",
    "nine_layer.add(Dropout(0.5))\n",
    "nine_layer.add(Dense(200, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "nine_layer.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=[\"accuracy\"])\n",
    "nine_layer.load_weights(\"nine_weights_augment_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************** Batch starting at 0 ********************\n",
      "shallow\n",
      "Epoch 1/10\n",
      "40200/40200 [==============================] - 22s - loss: 4.2512 - acc: 0.1627    \n",
      "Epoch 2/10\n",
      "40200/40200 [==============================] - 18s - loss: 1.8788 - acc: 0.5654    \n",
      "Epoch 3/10\n",
      "40200/40200 [==============================] - 18s - loss: 0.4730 - acc: 0.8907    \n",
      "Epoch 4/10\n",
      "40200/40200 [==============================] - 18s - loss: 0.0910 - acc: 0.9848    \n",
      "Epoch 5/10\n",
      "40200/40200 [==============================] - 18s - loss: 0.0281 - acc: 0.9975    \n",
      "Epoch 6/10\n",
      "40200/40200 [==============================] - 18s - loss: 0.0174 - acc: 0.9990    \n",
      "Epoch 7/10\n",
      "40200/40200 [==============================] - 18s - loss: 0.0139 - acc: 0.9993    \n",
      "Epoch 8/10\n",
      "40200/40200 [==============================] - 17s - loss: 0.0127 - acc: 0.9994    \n",
      "Epoch 9/10\n",
      "40200/40200 [==============================] - 17s - loss: 0.0122 - acc: 0.9994    \n",
      "Epoch 10/10\n",
      "40200/40200 [==============================] - 17s - loss: 0.0118 - acc: 0.9994    \n",
      "six_layer\n",
      "Epoch 1/10\n",
      "40200/40200 [==============================] - 27s - loss: 2.7581 - acc: 0.3653    \n",
      "Epoch 2/10\n",
      "40200/40200 [==============================] - 25s - loss: 0.7411 - acc: 0.8034    \n",
      "Epoch 3/10\n",
      "40200/40200 [==============================] - 25s - loss: 0.1472 - acc: 0.9566    \n",
      "Epoch 4/10\n",
      "40200/40200 [==============================] - 26s - loss: 0.0776 - acc: 0.9766    \n",
      "Epoch 5/10\n",
      "40200/40200 [==============================] - 25s - loss: 0.0644 - acc: 0.9799    \n",
      "Epoch 6/10\n",
      "40200/40200 [==============================] - 26s - loss: 0.0485 - acc: 0.9844    \n",
      "Epoch 7/10\n",
      "40200/40200 [==============================] - 25s - loss: 0.0439 - acc: 0.9858    \n",
      "Epoch 8/10\n",
      "40200/40200 [==============================] - 25s - loss: 0.0360 - acc: 0.9886    \n",
      "Epoch 9/10\n",
      "40200/40200 [==============================] - 25s - loss: 0.0302 - acc: 0.9912    \n",
      "Epoch 10/10\n",
      "40200/40200 [==============================] - 25s - loss: 0.0301 - acc: 0.9900    \n",
      "nine_layer\n",
      "Epoch 1/10\n",
      "40200/40200 [==============================] - 28s - loss: 3.6623 - acc: 0.2163    \n",
      "Epoch 2/10\n",
      "40200/40200 [==============================] - 28s - loss: 2.8636 - acc: 0.3422    \n",
      "Epoch 3/10\n",
      "40200/40200 [==============================] - 28s - loss: 2.2218 - acc: 0.4624    \n",
      "Epoch 4/10\n",
      "40200/40200 [==============================] - 28s - loss: 1.5695 - acc: 0.5978    \n",
      "Epoch 5/10\n",
      "40200/40200 [==============================] - 28s - loss: 1.0629 - acc: 0.7116    \n",
      "Epoch 6/10\n",
      "40200/40200 [==============================] - 28s - loss: 0.7444 - acc: 0.7873    \n",
      "Epoch 7/10\n",
      "40200/40200 [==============================] - 28s - loss: 0.5754 - acc: 0.8350    \n",
      "Epoch 8/10\n",
      "40200/40200 [==============================] - 28s - loss: 0.4630 - acc: 0.8645    \n",
      "Epoch 9/10\n",
      "40200/40200 [==============================] - 28s - loss: 0.3886 - acc: 0.8850    \n",
      "Epoch 10/10\n",
      "40200/40200 [==============================] - 28s - loss: 0.3659 - acc: 0.8936    \n",
      "**************** Batch starting at 200 ********************\n",
      "shallow\n",
      "Epoch 1/10\n",
      "40200/40200 [==============================] - 17s - loss: 4.1504 - acc: 0.1742    \n",
      "Epoch 2/10\n",
      "40200/40200 [==============================] - 17s - loss: 1.7105 - acc: 0.5980    \n",
      "Epoch 3/10\n",
      "40200/40200 [==============================] - 17s - loss: 0.4352 - acc: 0.9008    \n",
      "Epoch 4/10\n",
      "40200/40200 [==============================] - 17s - loss: 0.0911 - acc: 0.9858    \n",
      "Epoch 5/10\n",
      "40200/40200 [==============================] - 17s - loss: 0.0309 - acc: 0.9973    \n",
      "Epoch 6/10\n",
      "40200/40200 [==============================] - 17s - loss: 0.0180 - acc: 0.9990    \n",
      "Epoch 7/10\n",
      "40200/40200 [==============================] - 17s - loss: 0.0155 - acc: 0.9992    \n",
      "Epoch 8/10\n",
      "40200/40200 [==============================] - 17s - loss: 0.0139 - acc: 0.9993    \n",
      "Epoch 9/10\n",
      "40200/40200 [==============================] - 17s - loss: 0.0136 - acc: 0.9993    \n",
      "Epoch 10/10\n",
      "40200/40200 [==============================] - 17s - loss: 0.0131 - acc: 0.9993    \n",
      "six_layer\n",
      "Epoch 1/10\n",
      "40200/40200 [==============================] - 25s - loss: 2.5998 - acc: 0.3934    \n",
      "Epoch 2/10\n",
      "40200/40200 [==============================] - 25s - loss: 0.6612 - acc: 0.8210    \n",
      "Epoch 3/10\n",
      "40200/40200 [==============================] - 25s - loss: 0.1433 - acc: 0.9571    \n",
      "Epoch 4/10\n",
      "40200/40200 [==============================] - 25s - loss: 0.0842 - acc: 0.9737    \n",
      "Epoch 5/10\n",
      "40200/40200 [==============================] - 25s - loss: 0.0638 - acc: 0.9812    \n",
      "Epoch 6/10\n",
      "40200/40200 [==============================] - 25s - loss: 0.0500 - acc: 0.9851    \n",
      "Epoch 7/10\n",
      "40200/40200 [==============================] - 25s - loss: 0.0429 - acc: 0.9877    \n",
      "Epoch 8/10\n",
      "40200/40200 [==============================] - 25s - loss: 0.0412 - acc: 0.9878    \n",
      "Epoch 9/10\n",
      "40200/40200 [==============================] - 25s - loss: 0.0308 - acc: 0.9912    \n",
      "Epoch 10/10\n",
      "40200/40200 [==============================] - 25s - loss: 0.0296 - acc: 0.9913    \n",
      "nine_layer\n",
      "Epoch 1/10\n",
      "40200/40200 [==============================] - 27s - loss: 3.5909 - acc: 0.2306    \n",
      "Epoch 2/10\n",
      "40200/40200 [==============================] - 27s - loss: 2.8505 - acc: 0.3453    \n",
      "Epoch 3/10\n",
      "40200/40200 [==============================] - 27s - loss: 2.2614 - acc: 0.4532    \n",
      "Epoch 4/10\n",
      "40200/40200 [==============================] - 29s - loss: 1.6671 - acc: 0.5790    \n",
      "Epoch 5/10\n",
      "40200/40200 [==============================] - 28s - loss: 1.1703 - acc: 0.6888    \n",
      "Epoch 6/10\n",
      "40200/40200 [==============================] - 28s - loss: 0.7897 - acc: 0.7771    \n",
      "Epoch 7/10\n",
      "40200/40200 [==============================] - 28s - loss: 0.6307 - acc: 0.8182    \n",
      "Epoch 8/10\n",
      "40200/40200 [==============================] - 28s - loss: 0.5265 - acc: 0.8462    \n",
      "Epoch 9/10\n",
      "40200/40200 [==============================] - 29s - loss: 0.4365 - acc: 0.8731    \n",
      "Epoch 10/10\n",
      "40200/40200 [==============================] - 28s - loss: 0.3824 - acc: 0.8881    \n",
      "**************** Batch starting at 400 ********************\n",
      "shallow\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 9s - loss: 4.2753 - acc: 0.1628     \n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 9s - loss: 1.5699 - acc: 0.6367     \n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 8s - loss: 0.3649 - acc: 0.9245     \n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 8s - loss: 0.0903 - acc: 0.9868     \n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 8s - loss: 0.0376 - acc: 0.9968     \n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 9s - loss: 0.0269 - acc: 0.9985     \n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 9s - loss: 0.0230 - acc: 0.9987     \n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 9s - loss: 0.0207 - acc: 0.9988     \n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 9s - loss: 0.0200 - acc: 0.9988     \n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 9s - loss: 0.0195 - acc: 0.9988     \n",
      "six_layer\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 13s - loss: 2.7246 - acc: 0.3754    \n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 13s - loss: 0.6351 - acc: 0.8325    \n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 13s - loss: 0.0928 - acc: 0.9735    \n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 13s - loss: 0.0438 - acc: 0.9879    \n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 13s - loss: 0.0340 - acc: 0.9898    \n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 13s - loss: 0.0346 - acc: 0.9905    \n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 13s - loss: 0.0221 - acc: 0.9936    \n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 13s - loss: 0.0190 - acc: 0.9946    \n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 13s - loss: 0.0189 - acc: 0.9950    \n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 13s - loss: 0.0131 - acc: 0.9967    \n",
      "nine_layer\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 14s - loss: 3.7218 - acc: 0.2182    \n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 14s - loss: 2.9317 - acc: 0.3348    \n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 14s - loss: 2.2731 - acc: 0.4559    \n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 14s - loss: 1.5674 - acc: 0.5988    \n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 14s - loss: 0.9854 - acc: 0.7345    \n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 14s - loss: 0.6447 - acc: 0.8178    \n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 14s - loss: 0.4304 - acc: 0.8735    \n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 14s - loss: 0.3322 - acc: 0.9006    \n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 14s - loss: 0.2597 - acc: 0.9215    \n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 14s - loss: 0.2300 - acc: 0.9289    \n",
      "9696/9832 [============================>.] - ETA: 0s\n",
      "loss:7.79694572942\n",
      "acc:0.139239218877\n",
      "\n",
      "9792/9832 [============================>.] - ETA: 0s\n",
      "loss:7.84691687383\n",
      "acc:0.171379170057\n",
      "\n",
      "9824/9832 [============================>.] - ETA: 0s\n",
      "loss:6.41327486861\n",
      "acc:0.216029292107\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_le = None\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "for start_idx in [0,200,400]:\n",
    "    print \"**************** Batch starting at\", str(start_idx), \"********************\"\n",
    "\n",
    "    try:\n",
    "        del x_train\n",
    "        del y_train\n",
    "        del training_images\n",
    "        del training_labels\n",
    "        del training_files\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    x_train, training_labels, training_files = load_training_images(start_idx, batch_size=200)\n",
    "    \n",
    "    if train_le == None:\n",
    "        train_le = preprocessing.LabelEncoder() # Fit label encoder\n",
    "        train_le.fit(training_labels)\n",
    "\n",
    "    y_train = get_one_hot(train_le, training_labels)\n",
    "\n",
    "    print \"shallow\"\n",
    "    shallow.fit(x_train, y_train, batch_size=64, epochs=NUM_EPOCHS)\n",
    "    \n",
    "    print \"six_layer\"\n",
    "    six_layer.fit(x_train, y_train, batch_size=64, epochs=NUM_EPOCHS)\n",
    "    \n",
    "    print \"nine_layer\"\n",
    "    nine_layer.fit(x_train, y_train, batch_size=64, epochs=NUM_EPOCHS)\n",
    "    \n",
    "x_val, val_labels = load_validation_images()\n",
    "y_val = get_one_hot(train_le, val_labels)\n",
    "\n",
    "for model in [shallow, six_layer, nine_layer]:\n",
    "    score = model.evaluate(x_val, y_val, batch_size=32)\n",
    "    print\"\"\n",
    "    for x in range(0, len(model.metrics_names)):\n",
    "        print str(model.metrics_names[x]) + \":\" + str(score[x])\n",
    "    print \"\"\n",
    "\n",
    "shallow.save_weights(\"shallow_weights_augment_3\")\n",
    "six_layer.save_weights(\"six_weights_augment_3\")\n",
    "nine_layer.save_weights(\"nine_weights_augment_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### First Batch of 200\n",
    "training_images, training_labels, training_files = load_training_images(start=0, batch_size=200)\n",
    "#x_train, training_labels, training_files = randomize(training_images, training_labels, training_files)\n",
    "x_train = training_images\n",
    "train_le = preprocessing.LabelEncoder() # Fit label encoder\n",
    "train_le.fit(training_labels)\n",
    "\n",
    "y_train = get_one_hot(train_le, training_labels)\n",
    "\n",
    "shallow.fit(x_train, y_train, batch_size=64, epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model.fit(x_train, y_train, batch_size=32, epochs=1)\n",
    "######### 2nd Batch of 200\n",
    "try:\n",
    "    del x_train\n",
    "    del y_train\n",
    "    del training_images\n",
    "    del training_labels\n",
    "    del training_files\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "try:\n",
    "    print x_train\n",
    "    print training_images\n",
    "except:\n",
    "    print \"x_train and training_images not found!\"\n",
    "\n",
    "training_images, training_labels, training_files = load_training_images(start=200, batch_size=200)\n",
    "print \"after loading\"\n",
    "#x_train, training_labels, training_files = randomize(training_images, training_labels, training_files)\n",
    "x_train = training_images\n",
    "print \"after randomize\"\n",
    "y_train = get_one_hot(train_le, training_labels)\n",
    "print \"after one_hot\"\n",
    "\n",
    "shallow.fit(x_train, y_train, batch_size=64, epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "######## 3rd Batch of 100\n",
    "try:\n",
    "    del x_train\n",
    "    del y_train\n",
    "    del training_images\n",
    "    del training_labels\n",
    "    del training_files\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "try:\n",
    "    print x_train\n",
    "    print training_images\n",
    "except:\n",
    "    print \"x_train and training_images not found!\"\n",
    "\n",
    "training_images, training_labels, training_files = load_training_images(start=400, batch_size=200)\n",
    "#x_train, training_labels, training_files = randomize(training_images, training_labels, training_files)\n",
    "x_train = training_images\n",
    "y_train = get_one_hot(train_le, training_labels)\n",
    "\n",
    "shallow.fit(x_train, y_train, batch_size=64, epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val, val_labels = load_validation_images()\n",
    "y_val = get_one_hot(train_le, val_labels)\n",
    "\n",
    "score = model.evaluate(x_val, y_val, batch_size=32)\n",
    "print\"\"\n",
    "for x in range(0, len(model.metrics_names)):\n",
    "    print str(model.metrics_names[x]) + \":\" + str(score[x])\n",
    "print \"\"\n",
    "\n",
    "score = model.evaluate(x_train, y_train, batch_size=32)\n",
    "print \"\"\n",
    "\n",
    "for x in range(0, len(model.metrics_names)):\n",
    "    print str(model.metrics_names[x]) + \":\" + str(score[x])\n",
    "    \n",
    "# without dropout: acc:0.177"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
