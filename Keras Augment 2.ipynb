{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import os\n",
    "import matplotlib.image as mpimg\n",
    "import cv2\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "TRAIN_PATH = \"/datasets/tmp/aayang/tiny-imagenet-200/train/\"\n",
    "VAL_IMAGES_PATH = \"/datasets/tmp/aayang/tiny-imagenet-200/val/images/\"\n",
    "VAL_ANNO_FILE = \"/datasets/tmp/aayang/tiny-imagenet-200/val/val_annotations.txt\"\n",
    "TRAINING_IMAGE_SIZE = 64\n",
    "\n",
    "LOAD_BATCH_SIZE = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_images(start=0, batch_size=200):\n",
    "    images = [] #np.ndarray(shape=(batch_size*200, 64,64,3))\n",
    "    labels = []\n",
    "    file_names = []\n",
    "    class_num = 0\n",
    "    \n",
    "    for class_folder in os.listdir(TRAIN_PATH):\n",
    "        images_folder = TRAIN_PATH + class_folder + \"/flipped/\"\n",
    "        x = 0\n",
    "        \n",
    "        for image_name in os.listdir(images_folder):\n",
    "            if x < start:\n",
    "                x += 1\n",
    "                continue\n",
    "            \n",
    "            image_path = os.path.join(images_folder, image_name)\n",
    "            image = mpimg.imread(image_path)\n",
    "            \n",
    "            if image.shape == (TRAINING_IMAGE_SIZE, TRAINING_IMAGE_SIZE, 3):\n",
    "                images.append(image/256.0)\n",
    "                labels.append(class_folder)\n",
    "                file_names.append(image_name)\n",
    "    \n",
    "            x += 1      \n",
    "            #if x%100 == 0: \n",
    "            #    print \"class\", class_num, \"-\", x\n",
    "                \n",
    "            if x > start+batch_size:\n",
    "                break\n",
    "        class_num += 1\n",
    "                        \n",
    "    return (np.array(images), np.array(labels), np.array(file_names))\n",
    "\n",
    "def get_one_hot(le, labels):\n",
    "    labels_encoded = le.transform(labels)\n",
    "    y = keras.utils.to_categorical(labels_encoded)\n",
    "    return y\n",
    "    \n",
    "def randomize(images, labels, files):\n",
    "    shuffle_index = np.random.permutation(len(images))\n",
    "    \n",
    "    shuffled_images = images[shuffle_index]\n",
    "    shuffled_labels = labels[shuffle_index]\n",
    "    shuffled_files  = files[shuffle_index]\n",
    "    \n",
    "    return (shuffled_images, shuffled_labels, shuffled_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_validation_images():\n",
    "    with open(VAL_ANNO_FILE) as f:\n",
    "        lines = f.read().splitlines()\n",
    "        \n",
    "    labels = []\n",
    "    images = []\n",
    "    idx = 0\n",
    "    \n",
    "    for line in lines:\n",
    "        image_att = line.split(\"\\t\")\n",
    "        image_name = image_att[0]\n",
    "        image_label = image_att[1]\n",
    "        \n",
    "        image_path = os.path.join(VAL_IMAGES_PATH, image_name)\n",
    "        image = mpimg.imread(image_path)\n",
    "        \n",
    "        if image.shape == (TRAINING_IMAGE_SIZE, TRAINING_IMAGE_SIZE, 3):\n",
    "            images.append(image/256.0)\n",
    "            labels.append(image_label)\n",
    "            \n",
    "    return (np.array(images), np.array(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Layer CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "shallow = Sequential()\n",
    "# input: 100x100 images with 3 channels -> (100, 100, 3) tensors.\n",
    "# this applies 32 convolution filters of size 3x3 each.\n",
    "shallow.add(Conv2D(64, (3, 3), activation='relu', input_shape=(64,64,3)))\n",
    "shallow.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#shallow.add(Conv2D(128, (3, 3), activation='relu', padding=\"same\"))\n",
    "#shallow.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#shallow.add(Conv2D(256, (5, 5), activation='relu', padding=\"same\"))\n",
    "#shallow.add(Conv2D(256, (5, 5), activation='relu', padding=\"same\"))\n",
    "#shallow.add(Conv2D(256, (5, 5), activation='relu', padding=\"same\"))\n",
    "#shallow.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "shallow.add(Flatten())\n",
    "shallow.add(Dense(1024, activation='relu'))\n",
    "#shallow.add(Dense(1024, activation='relu'))\n",
    "#shallow.add(Dropout(0.5))\n",
    "#shallow.add(Dense(256, activation='relu'))\n",
    "shallow.add(Dense(200, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "shallow.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=[\"accuracy\"])\n",
    "shallow.load_weights(\"shallow_weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Layer CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "six_layer = Sequential()\n",
    "\n",
    "six_layer.add(Conv2D(64, (3, 3), activation='relu', input_shape=(64,64,3)))\n",
    "six_layer.add(Conv2D(64, (3, 3), activation='relu', padding=\"same\"))\n",
    "six_layer.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "six_layer.add(Conv2D(128, (3, 3), activation='relu', padding=\"same\"))\n",
    "six_layer.add(Conv2D(128, (3, 3), activation='relu', padding=\"same\"))\n",
    "six_layer.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "six_layer.add(Dropout(0.25))\n",
    "\n",
    "#six_layer.add(Conv2D(256, (5, 5), activation='relu', padding=\"same\"))\n",
    "#six_layer.add(Conv2D(256, (5, 5), activation='relu', padding=\"same\"))\n",
    "#six_layer.add(Conv2D(256, (5, 5), activation='relu', padding=\"same\"))\n",
    "#six_layer.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "six_layer.add(Flatten())\n",
    "six_layer.add(Dense(1024, activation='relu'))\n",
    "#six_layer.add(Dense(1024, activation='relu'))\n",
    "#six_layer.add(Dropout(0.5))\n",
    "#six_layer.add(Dense(256, activation='relu'))\n",
    "six_layer.add(Dense(200, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "six_layer.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=[\"accuracy\"])\n",
    "six_layer.load_weights(\"six_weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9 Layer CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nine_layer = Sequential()\n",
    "# input: 100x100 images with 3 channels -> (100, 100, 3) tensors.\n",
    "# this applies 32 convolution filters of size 3x3 each.\n",
    "nine_layer.add(Conv2D(64, (3, 3), activation='relu', input_shape=(64,64,3)))\n",
    "nine_layer.add(Conv2D(64, (3, 3), activation='relu', padding=\"same\"))\n",
    "nine_layer.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "nine_layer.add(Conv2D(128, (3, 3), activation='relu', padding=\"same\"))\n",
    "nine_layer.add(Conv2D(128, (3, 3), activation='relu', padding=\"same\"))\n",
    "nine_layer.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "nine_layer.add(Dropout(0.25))\n",
    "\n",
    "nine_layer.add(Conv2D(256, (3, 3), activation='relu', padding=\"same\"))\n",
    "nine_layer.add(Conv2D(256, (3, 3), activation='relu', padding=\"same\"))\n",
    "nine_layer.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "nine_layer.add(Flatten())\n",
    "nine_layer.add(Dense(1024, activation='relu'))\n",
    "nine_layer.add(Dense(1024, activation='relu'))\n",
    "nine_layer.add(Dropout(0.5))\n",
    "nine_layer.add(Dense(200, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "nine_layer.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=[\"accuracy\"])\n",
    "nine_layer.load_weights(\"nine_weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************** Batch starting at 0 ********************\n",
      "shallow\n",
      "Epoch 1/10\n",
      "40200/40200 [==============================] - 21s - loss: 4.1414 - acc: 0.1504    \n",
      "Epoch 2/10\n",
      "40200/40200 [==============================] - 18s - loss: 2.3624 - acc: 0.4504    \n",
      "Epoch 3/10\n",
      "40200/40200 [==============================] - 18s - loss: 0.8460 - acc: 0.8091    \n",
      "Epoch 4/10\n",
      "40200/40200 [==============================] - 18s - loss: 0.1863 - acc: 0.9671    \n",
      "Epoch 5/10\n",
      "40200/40200 [==============================] - 18s - loss: 0.0524 - acc: 0.9927    \n",
      "Epoch 6/10\n",
      "40200/40200 [==============================] - 17s - loss: 0.0238 - acc: 0.9974    \n",
      "Epoch 7/10\n",
      "40200/40200 [==============================] - 18s - loss: 0.0135 - acc: 0.9990    \n",
      "Epoch 8/10\n",
      "40200/40200 [==============================] - 18s - loss: 0.0104 - acc: 0.9994    \n",
      "Epoch 9/10\n",
      "40200/40200 [==============================] - 18s - loss: 0.0088 - acc: 0.9996    \n",
      "Epoch 10/10\n",
      "40200/40200 [==============================] - 18s - loss: 0.0080 - acc: 0.9996    \n",
      "six_layer\n",
      "Epoch 1/10\n",
      "40200/40200 [==============================] - 27s - loss: 3.4969 - acc: 0.2376    \n",
      "Epoch 2/10\n",
      "40200/40200 [==============================] - 26s - loss: 2.0319 - acc: 0.5054    \n",
      "Epoch 3/10\n",
      "40200/40200 [==============================] - 26s - loss: 0.4852 - acc: 0.8665    \n",
      "Epoch 4/10\n",
      "40200/40200 [==============================] - 26s - loss: 0.1355 - acc: 0.9599    \n",
      "Epoch 5/10\n",
      "40200/40200 [==============================] - 25s - loss: 0.0923 - acc: 0.9720    \n",
      "Epoch 6/10\n",
      "40200/40200 [==============================] - 26s - loss: 0.0689 - acc: 0.9794    \n",
      "Epoch 7/10\n",
      "40200/40200 [==============================] - 26s - loss: 0.0619 - acc: 0.9814    \n",
      "Epoch 8/10\n",
      "40200/40200 [==============================] - 25s - loss: 0.0517 - acc: 0.9845    \n",
      "Epoch 9/10\n",
      "40200/40200 [==============================] - 25s - loss: 0.0461 - acc: 0.9862    \n",
      "Epoch 10/10\n",
      "40200/40200 [==============================] - 26s - loss: 0.0403 - acc: 0.9883    \n",
      "nine_layer\n",
      "Epoch 1/10\n",
      "40200/40200 [==============================] - 28s - loss: 3.5763 - acc: 0.2194    \n",
      "Epoch 2/10\n",
      "40200/40200 [==============================] - 28s - loss: 2.8158 - acc: 0.3467    \n",
      "Epoch 3/10\n",
      "40200/40200 [==============================] - 28s - loss: 2.1721 - acc: 0.4683    \n",
      "Epoch 4/10\n",
      "40200/40200 [==============================] - 28s - loss: 1.5223 - acc: 0.6026    \n",
      "Epoch 5/10\n",
      "40200/40200 [==============================] - 28s - loss: 1.0068 - acc: 0.7229    \n",
      "Epoch 6/10\n",
      "40200/40200 [==============================] - 28s - loss: 0.7207 - acc: 0.7951    \n",
      "Epoch 7/10\n",
      "40200/40200 [==============================] - 28s - loss: 0.5705 - acc: 0.8358    \n",
      "Epoch 8/10\n",
      "40200/40200 [==============================] - 28s - loss: 0.4707 - acc: 0.8621    \n",
      "Epoch 9/10\n",
      "40200/40200 [==============================] - 27s - loss: 0.3767 - acc: 0.8898    \n",
      "Epoch 10/10\n",
      "40200/40200 [==============================] - 28s - loss: 0.3477 - acc: 0.8972    \n",
      "**************** Batch starting at 200 ********************\n",
      "shallow\n",
      "Epoch 1/10\n",
      "40200/40200 [==============================] - 17s - loss: 4.1843 - acc: 0.1486    \n",
      "Epoch 2/10\n",
      "40200/40200 [==============================] - 17s - loss: 2.3835 - acc: 0.4448    \n",
      "Epoch 3/10\n",
      "40200/40200 [==============================] - 17s - loss: 0.8711 - acc: 0.8036    \n",
      "Epoch 4/10\n",
      "40200/40200 [==============================] - 17s - loss: 0.1941 - acc: 0.9660    \n",
      "Epoch 5/10\n",
      "40200/40200 [==============================] - 17s - loss: 0.0546 - acc: 0.9934    \n",
      "Epoch 6/10\n",
      "40200/40200 [==============================] - 16s - loss: 0.0259 - acc: 0.9975    \n",
      "Epoch 7/10\n",
      "40200/40200 [==============================] - 17s - loss: 0.0181 - acc: 0.9986    \n",
      "Epoch 8/10\n",
      "40200/40200 [==============================] - 17s - loss: 0.0138 - acc: 0.9991    \n",
      "Epoch 9/10\n",
      "40200/40200 [==============================] - 16s - loss: 0.0108 - acc: 0.9996    \n",
      "Epoch 10/10\n",
      "40200/40200 [==============================] - 17s - loss: 0.0104 - acc: 0.9995    \n",
      "six_layer\n",
      "Epoch 1/10\n",
      "40200/40200 [==============================] - 25s - loss: 3.5176 - acc: 0.2398    \n",
      "Epoch 2/10\n",
      "40200/40200 [==============================] - 25s - loss: 2.1092 - acc: 0.4919    \n",
      "Epoch 3/10\n",
      "40200/40200 [==============================] - 25s - loss: 0.5931 - acc: 0.8397    \n",
      "Epoch 4/10\n",
      "40200/40200 [==============================] - 25s - loss: 0.1296 - acc: 0.9614    \n",
      "Epoch 5/10\n",
      "40200/40200 [==============================] - 25s - loss: 0.0863 - acc: 0.9734    \n",
      "Epoch 6/10\n",
      "40200/40200 [==============================] - 24s - loss: 0.0630 - acc: 0.9809    \n",
      "Epoch 7/10\n",
      "40200/40200 [==============================] - 24s - loss: 0.0467 - acc: 0.9857    \n",
      "Epoch 8/10\n",
      "40200/40200 [==============================] - 24s - loss: 0.0387 - acc: 0.9888    \n",
      "Epoch 9/10\n",
      "40200/40200 [==============================] - 25s - loss: 0.0397 - acc: 0.9880    \n",
      "Epoch 10/10\n",
      "40200/40200 [==============================] - 25s - loss: 0.0326 - acc: 0.9901    \n",
      "nine_layer\n",
      "Epoch 1/10\n",
      "40200/40200 [==============================] - 27s - loss: 3.6024 - acc: 0.2248    \n",
      "Epoch 2/10\n",
      "40200/40200 [==============================] - 27s - loss: 2.8630 - acc: 0.3401    \n",
      "Epoch 3/10\n",
      "40200/40200 [==============================] - 27s - loss: 2.2584 - acc: 0.4562    \n",
      "Epoch 4/10\n",
      "40200/40200 [==============================] - 27s - loss: 1.6268 - acc: 0.5843    \n",
      "Epoch 5/10\n",
      "40200/40200 [==============================] - 27s - loss: 1.0908 - acc: 0.7035    \n",
      "Epoch 6/10\n",
      "40200/40200 [==============================] - 27s - loss: 0.7453 - acc: 0.7872    \n",
      "Epoch 7/10\n",
      "40200/40200 [==============================] - 27s - loss: 0.5696 - acc: 0.8326    \n",
      "Epoch 8/10\n",
      "40200/40200 [==============================] - 27s - loss: 0.4736 - acc: 0.8623    \n",
      "Epoch 9/10\n",
      "40200/40200 [==============================] - 27s - loss: 0.3927 - acc: 0.8831    \n",
      "Epoch 10/10\n",
      "40200/40200 [==============================] - 27s - loss: 0.3382 - acc: 0.8999    \n",
      "**************** Batch starting at 400 ********************\n",
      "shallow\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 9s - loss: 4.3691 - acc: 0.1374     \n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 9s - loss: 2.2686 - acc: 0.4823     \n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 9s - loss: 0.7130 - acc: 0.8470     \n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 9s - loss: 0.1407 - acc: 0.9809     \n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 9s - loss: 0.0451 - acc: 0.9959     \n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 9s - loss: 0.0252 - acc: 0.9986     \n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 9s - loss: 0.0186 - acc: 0.9992     \n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 8s - loss: 0.0160 - acc: 0.9992     \n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 9s - loss: 0.0151 - acc: 0.9994     \n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 9s - loss: 0.0137 - acc: 0.9994     \n",
      "six_layer\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 12s - loss: 3.6896 - acc: 0.2209    \n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 13s - loss: 1.9672 - acc: 0.5240    \n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 12s - loss: 0.4206 - acc: 0.8914    \n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 13s - loss: 0.0726 - acc: 0.9806    \n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 13s - loss: 0.0343 - acc: 0.9909    \n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 13s - loss: 0.0197 - acc: 0.9953    \n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 13s - loss: 0.0189 - acc: 0.9950    \n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 13s - loss: 0.0188 - acc: 0.9953    \n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 13s - loss: 0.0141 - acc: 0.9968    \n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 13s - loss: 0.0124 - acc: 0.9969    \n",
      "nine_layer\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 13s - loss: 3.7346 - acc: 0.2161    \n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 13s - loss: 2.8497 - acc: 0.3493    \n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 13s - loss: 2.1067 - acc: 0.4869    \n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 14s - loss: 1.3388 - acc: 0.6512    \n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 14s - loss: 0.7595 - acc: 0.7890    \n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 14s - loss: 0.4804 - acc: 0.8585    \n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 14s - loss: 0.3465 - acc: 0.8950    \n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 14s - loss: 0.2625 - acc: 0.9216    \n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 14s - loss: 0.2106 - acc: 0.9370    \n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 14s - loss: 0.2007 - acc: 0.9389    \n",
      "9472/9832 [===========================>..] - ETA: 0s\n",
      "loss:6.162317683\n",
      "acc:0.171989422295\n",
      "\n",
      "9832/9832 [==============================] - 2s     \n",
      "\n",
      "loss:6.95128935697\n",
      "acc:0.23637103336\n",
      "\n",
      "9696/9832 [============================>.] - ETA: 0s\n",
      "loss:5.93463866627\n",
      "acc:0.241863303499\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_le = None\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "for start_idx in [0,200,400]:\n",
    "    print \"**************** Batch starting at\", str(start_idx), \"********************\"\n",
    "\n",
    "    try:\n",
    "        del x_train\n",
    "        del y_train\n",
    "        del training_images\n",
    "        del training_labels\n",
    "        del training_files\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    x_train, training_labels, training_files = load_training_images(start_idx, batch_size=200)\n",
    "    \n",
    "    if train_le == None:\n",
    "        train_le = preprocessing.LabelEncoder() # Fit label encoder\n",
    "        train_le.fit(training_labels)\n",
    "\n",
    "    y_train = get_one_hot(train_le, training_labels)\n",
    "\n",
    "    print \"shallow\"\n",
    "    shallow.fit(x_train, y_train, batch_size=64, epochs=NUM_EPOCHS)\n",
    "    \n",
    "    print \"six_layer\"\n",
    "    six_layer.fit(x_train, y_train, batch_size=64, epochs=NUM_EPOCHS)\n",
    "    \n",
    "    print \"nine_layer\"\n",
    "    nine_layer.fit(x_train, y_train, batch_size=64, epochs=NUM_EPOCHS)\n",
    "    \n",
    "x_val, val_labels = load_validation_images()\n",
    "y_val = get_one_hot(train_le, val_labels)\n",
    "\n",
    "for model in [shallow, six_layer, nine_layer]:\n",
    "    score = model.evaluate(x_val, y_val, batch_size=32)\n",
    "    print\"\"\n",
    "    for x in range(0, len(model.metrics_names)):\n",
    "        print str(model.metrics_names[x]) + \":\" + str(score[x])\n",
    "    print \"\"\n",
    "\n",
    "shallow.save_weights(\"shallow_weights_augment_2\")\n",
    "six_layer.save_weights(\"six_weights_augment_2\")\n",
    "nine_layer.save_weights(\"nine_weights_augment_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shallow.save_weights(\"shallow_weights_augment_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### First Batch of 200\n",
    "training_images, training_labels, training_files = load_training_images(start=0, batch_size=200)\n",
    "#x_train, training_labels, training_files = randomize(training_images, training_labels, training_files)\n",
    "x_train = training_images\n",
    "train_le = preprocessing.LabelEncoder() # Fit label encoder\n",
    "train_le.fit(training_labels)\n",
    "\n",
    "y_train = get_one_hot(train_le, training_labels)\n",
    "\n",
    "shallow.fit(x_train, y_train, batch_size=64, epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model.fit(x_train, y_train, batch_size=32, epochs=1)\n",
    "######### 2nd Batch of 200\n",
    "try:\n",
    "    del x_train\n",
    "    del y_train\n",
    "    del training_images\n",
    "    del training_labels\n",
    "    del training_files\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "try:\n",
    "    print x_train\n",
    "    print training_images\n",
    "except:\n",
    "    print \"x_train and training_images not found!\"\n",
    "\n",
    "training_images, training_labels, training_files = load_training_images(start=200, batch_size=200)\n",
    "print \"after loading\"\n",
    "#x_train, training_labels, training_files = randomize(training_images, training_labels, training_files)\n",
    "x_train = training_images\n",
    "print \"after randomize\"\n",
    "y_train = get_one_hot(train_le, training_labels)\n",
    "print \"after one_hot\"\n",
    "\n",
    "shallow.fit(x_train, y_train, batch_size=64, epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "######## 3rd Batch of 100\n",
    "try:\n",
    "    del x_train\n",
    "    del y_train\n",
    "    del training_images\n",
    "    del training_labels\n",
    "    del training_files\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "try:\n",
    "    print x_train\n",
    "    print training_images\n",
    "except:\n",
    "    print \"x_train and training_images not found!\"\n",
    "\n",
    "training_images, training_labels, training_files = load_training_images(start=400, batch_size=200)\n",
    "#x_train, training_labels, training_files = randomize(training_images, training_labels, training_files)\n",
    "x_train = training_images\n",
    "y_train = get_one_hot(train_le, training_labels)\n",
    "\n",
    "shallow.fit(x_train, y_train, batch_size=64, epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val, val_labels = load_validation_images()\n",
    "y_val = get_one_hot(train_le, val_labels)\n",
    "\n",
    "score = model.evaluate(x_val, y_val, batch_size=32)\n",
    "print\"\"\n",
    "for x in range(0, len(model.metrics_names)):\n",
    "    print str(model.metrics_names[x]) + \":\" + str(score[x])\n",
    "print \"\"\n",
    "\n",
    "score = model.evaluate(x_train, y_train, batch_size=32)\n",
    "print \"\"\n",
    "\n",
    "for x in range(0, len(model.metrics_names)):\n",
    "    print str(model.metrics_names[x]) + \":\" + str(score[x])\n",
    "    \n",
    "# without dropout: acc:0.177"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
