{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import os\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "TRAIN_PATH = \"/datasets/tmp/aayang/tiny-imagenet-200/train/\"\n",
    "VAL_IMAGES_PATH = \"/datasets/tmp/aayang/tiny-imagenet-200/val/images/\"\n",
    "VAL_ANNO_FILE = \"/datasets/tmp/aayang/tiny-imagenet-200/val/val_annotations.txt\"\n",
    "TRAINING_IMAGE_SIZE = 64\n",
    "\n",
    "LOAD_BATCH_SIZE = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_images(start=0, batch_size=200):\n",
    "    images = [] #np.ndarray(shape=(batch_size*200, 64,64,3))\n",
    "    labels = []\n",
    "    file_names = []\n",
    "    class_num = 0\n",
    "    \n",
    "    for class_folder in os.listdir(TRAIN_PATH):\n",
    "        images_folder = TRAIN_PATH + class_folder + \"/images/\"\n",
    "        x = 0\n",
    "        \n",
    "        for image_name in os.listdir(images_folder):\n",
    "            if x < start:\n",
    "                x += 1\n",
    "                continue\n",
    "            \n",
    "            image_path = os.path.join(images_folder, image_name)\n",
    "            image = mpimg.imread(image_path)\n",
    "            \n",
    "            if image.shape == (TRAINING_IMAGE_SIZE, TRAINING_IMAGE_SIZE, 3):\n",
    "                images.append(image/256.0)\n",
    "                labels.append(class_folder)\n",
    "                file_names.append(image_name)\n",
    "    \n",
    "            x += 1      \n",
    "            #if x%100 == 0: \n",
    "            #    print \"class\", class_num, \"-\", x\n",
    "                \n",
    "            if x > start+batch_size:\n",
    "                break\n",
    "        class_num += 1\n",
    "                        \n",
    "    return (np.array(images), np.array(labels), np.array(file_names))\n",
    "\n",
    "def get_one_hot(le, labels):\n",
    "    labels_encoded = le.transform(labels)\n",
    "    y = keras.utils.to_categorical(labels_encoded)\n",
    "    return y\n",
    "    \n",
    "def randomize(images, labels, files):\n",
    "    shuffle_index = np.random.permutation(len(images))\n",
    "    \n",
    "    shuffled_images = images[shuffle_index]\n",
    "    shuffled_labels = labels[shuffle_index]\n",
    "    shuffled_files  = files[shuffle_index]\n",
    "    \n",
    "    return (shuffled_images, shuffled_labels, shuffled_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_validation_images():\n",
    "    with open(VAL_ANNO_FILE) as f:\n",
    "        lines = f.read().splitlines()\n",
    "        \n",
    "    labels = []\n",
    "    images = []\n",
    "    idx = 0\n",
    "    \n",
    "    for line in lines:\n",
    "        image_att = line.split(\"\\t\")\n",
    "        image_name = image_att[0]\n",
    "        image_label = image_att[1]\n",
    "        \n",
    "        image_path = os.path.join(VAL_IMAGES_PATH, image_name)\n",
    "        image = mpimg.imread(image_path)\n",
    "        \n",
    "        if image.shape == (TRAINING_IMAGE_SIZE, TRAINING_IMAGE_SIZE, 3):\n",
    "            images.append(image/256.0)\n",
    "            labels.append(image_label)\n",
    "            \n",
    "    return (np.array(images), np.array(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Layer CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "shallow = Sequential()\n",
    "# input: 100x100 images with 3 channels -> (100, 100, 3) tensors.\n",
    "# this applies 32 convolution filters of size 3x3 each.\n",
    "shallow.add(Conv2D(64, (3, 3), activation='relu', input_shape=(64,64,3)))\n",
    "shallow.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#shallow.add(Conv2D(128, (3, 3), activation='relu', padding=\"same\"))\n",
    "#shallow.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#shallow.add(Conv2D(256, (5, 5), activation='relu', padding=\"same\"))\n",
    "#shallow.add(Conv2D(256, (5, 5), activation='relu', padding=\"same\"))\n",
    "#shallow.add(Conv2D(256, (5, 5), activation='relu', padding=\"same\"))\n",
    "#shallow.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "shallow.add(Flatten())\n",
    "shallow.add(Dense(1024, activation='relu'))\n",
    "#shallow.add(Dense(1024, activation='relu'))\n",
    "#shallow.add(Dropout(0.5))\n",
    "#shallow.add(Dense(256, activation='relu'))\n",
    "shallow.add(Dense(200, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "shallow.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Layer CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "six_layer = Sequential()\n",
    "\n",
    "six_layer.add(Conv2D(64, (3, 3), activation='relu', input_shape=(64,64,3)))\n",
    "six_layer.add(Conv2D(64, (3, 3), activation='relu', padding=\"same\"))\n",
    "six_layer.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "six_layer.add(Conv2D(128, (3, 3), activation='relu', padding=\"same\"))\n",
    "six_layer.add(Conv2D(128, (3, 3), activation='relu', padding=\"same\"))\n",
    "six_layer.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "six_layer.add(Dropout(0.25))\n",
    "\n",
    "#six_layer.add(Conv2D(256, (5, 5), activation='relu', padding=\"same\"))\n",
    "#six_layer.add(Conv2D(256, (5, 5), activation='relu', padding=\"same\"))\n",
    "#six_layer.add(Conv2D(256, (5, 5), activation='relu', padding=\"same\"))\n",
    "#six_layer.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "six_layer.add(Flatten())\n",
    "six_layer.add(Dense(1024, activation='relu'))\n",
    "#six_layer.add(Dense(1024, activation='relu'))\n",
    "#six_layer.add(Dropout(0.5))\n",
    "#six_layer.add(Dense(256, activation='relu'))\n",
    "six_layer.add(Dense(200, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "six_layer.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9 Layer CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nine_layer = Sequential()\n",
    "# input: 100x100 images with 3 channels -> (100, 100, 3) tensors.\n",
    "# this applies 32 convolution filters of size 3x3 each.\n",
    "nine_layer.add(Conv2D(64, (3, 3), activation='relu', input_shape=(64,64,3)))\n",
    "nine_layer.add(Conv2D(64, (3, 3), activation='relu', padding=\"same\"))\n",
    "nine_layer.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "nine_layer.add(Conv2D(128, (3, 3), activation='relu', padding=\"same\"))\n",
    "nine_layer.add(Conv2D(128, (3, 3), activation='relu', padding=\"same\"))\n",
    "nine_layer.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "nine_layer.add(Dropout(0.25))\n",
    "\n",
    "nine_layer.add(Conv2D(256, (3, 3), activation='relu', padding=\"same\"))\n",
    "nine_layer.add(Conv2D(256, (3, 3), activation='relu', padding=\"same\"))\n",
    "nine_layer.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "nine_layer.add(Flatten())\n",
    "nine_layer.add(Dense(1024, activation='relu'))\n",
    "nine_layer.add(Dense(1024, activation='relu'))\n",
    "nine_layer.add(Dropout(0.5))\n",
    "nine_layer.add(Dense(200, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "nine_layer.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************** Batch starting at 0 ********************\n",
      "shallow\n",
      "Epoch 1/10\n",
      "39451/39451 [==============================] - 25s - loss: 4.9551 - acc: 0.0370    \n",
      "Epoch 2/10\n",
      "39451/39451 [==============================] - 18s - loss: 4.2822 - acc: 0.1180    \n",
      "Epoch 3/10\n",
      "39451/39451 [==============================] - 18s - loss: 3.5715 - acc: 0.2257    \n",
      "Epoch 4/10\n",
      "39451/39451 [==============================] - 18s - loss: 2.6120 - acc: 0.4024    \n",
      "Epoch 5/10\n",
      "39451/39451 [==============================] - 18s - loss: 1.4208 - acc: 0.6609    \n",
      "Epoch 6/10\n",
      "39451/39451 [==============================] - 18s - loss: 0.5690 - acc: 0.8675    \n",
      "Epoch 7/10\n",
      "39451/39451 [==============================] - 18s - loss: 0.2214 - acc: 0.9540    \n",
      "Epoch 8/10\n",
      "39451/39451 [==============================] - 18s - loss: 0.1029 - acc: 0.9803    \n",
      "Epoch 9/10\n",
      "39451/39451 [==============================] - 17s - loss: 0.0489 - acc: 0.9922    \n",
      "Epoch 10/10\n",
      "39451/39451 [==============================] - 17s - loss: 0.0251 - acc: 0.9966    \n",
      "six_layer\n",
      "Epoch 1/10\n",
      "39451/39451 [==============================] - 27s - loss: 5.1362 - acc: 0.0181    \n",
      "Epoch 2/10\n",
      "39451/39451 [==============================] - 25s - loss: 4.6210 - acc: 0.0707    \n",
      "Epoch 3/10\n",
      "39451/39451 [==============================] - 25s - loss: 4.0289 - acc: 0.1504    \n",
      "Epoch 4/10\n",
      "39451/39451 [==============================] - 25s - loss: 3.4213 - acc: 0.2366    \n",
      "Epoch 5/10\n",
      "39451/39451 [==============================] - 25s - loss: 2.6590 - acc: 0.3739    \n",
      "Epoch 6/10\n",
      "39451/39451 [==============================] - 26s - loss: 1.5190 - acc: 0.6133    \n",
      "Epoch 7/10\n",
      "39451/39451 [==============================] - 25s - loss: 0.6000 - acc: 0.8340    \n",
      "Epoch 8/10\n",
      "39451/39451 [==============================] - 25s - loss: 0.3532 - acc: 0.8986    \n",
      "Epoch 9/10\n",
      "39451/39451 [==============================] - 25s - loss: 0.2273 - acc: 0.9327    \n",
      "Epoch 10/10\n",
      "39451/39451 [==============================] - 25s - loss: 0.1683 - acc: 0.9501    \n",
      "nine_layer\n",
      "Epoch 1/10\n",
      "39451/39451 [==============================] - 27s - loss: 5.2982 - acc: 0.0061    \n",
      "Epoch 2/10\n",
      "39451/39451 [==============================] - 27s - loss: 5.1637 - acc: 0.0132    \n",
      "Epoch 3/10\n",
      "39451/39451 [==============================] - 27s - loss: 5.0143 - acc: 0.0242    \n",
      "Epoch 4/10\n",
      "39451/39451 [==============================] - 27s - loss: 4.8490 - acc: 0.0383    \n",
      "Epoch 5/10\n",
      "39451/39451 [==============================] - 27s - loss: 4.6026 - acc: 0.0637    \n",
      "Epoch 6/10\n",
      "39451/39451 [==============================] - 28s - loss: 4.3140 - acc: 0.0989    \n",
      "Epoch 7/10\n",
      "39451/39451 [==============================] - 28s - loss: 4.0215 - acc: 0.1388    \n",
      "Epoch 8/10\n",
      "39451/39451 [==============================] - 28s - loss: 3.7329 - acc: 0.1780    \n",
      "Epoch 9/10\n",
      "39451/39451 [==============================] - 27s - loss: 3.4400 - acc: 0.2249    \n",
      "Epoch 10/10\n",
      "39451/39451 [==============================] - 28s - loss: 3.1171 - acc: 0.2800    \n",
      "**************** Batch starting at 200 ********************\n",
      "shallow\n",
      "Epoch 1/10\n",
      "39470/39470 [==============================] - 16s - loss: 4.2397 - acc: 0.1328    \n",
      "Epoch 2/10\n",
      "39470/39470 [==============================] - 16s - loss: 2.8660 - acc: 0.3519    \n",
      "Epoch 3/10\n",
      "39470/39470 [==============================] - 16s - loss: 1.4583 - acc: 0.6624    \n",
      "Epoch 4/10\n",
      "39470/39470 [==============================] - 16s - loss: 0.4402 - acc: 0.9102    \n",
      "Epoch 5/10\n",
      "39470/39470 [==============================] - 16s - loss: 0.1136 - acc: 0.9819    \n",
      "Epoch 6/10\n",
      "39470/39470 [==============================] - 16s - loss: 0.0396 - acc: 0.9951    \n",
      "Epoch 7/10\n",
      "39470/39470 [==============================] - 16s - loss: 0.0204 - acc: 0.9977    \n",
      "Epoch 8/10\n",
      "39470/39470 [==============================] - 16s - loss: 0.0125 - acc: 0.9991    \n",
      "Epoch 9/10\n",
      "39470/39470 [==============================] - 16s - loss: 0.0084 - acc: 0.9996    \n",
      "Epoch 10/10\n",
      "39470/39470 [==============================] - 16s - loss: 0.0063 - acc: 0.9998    \n",
      "six_layer\n",
      "Epoch 1/10\n",
      "39470/39470 [==============================] - 24s - loss: 3.7918 - acc: 0.1915    \n",
      "Epoch 2/10\n",
      "39470/39470 [==============================] - 24s - loss: 2.5346 - acc: 0.4029    \n",
      "Epoch 3/10\n",
      "39470/39470 [==============================] - 24s - loss: 0.9111 - acc: 0.7603    \n",
      "Epoch 4/10\n",
      "39470/39470 [==============================] - 24s - loss: 0.2290 - acc: 0.9324    \n",
      "Epoch 5/10\n",
      "39470/39470 [==============================] - 24s - loss: 0.1499 - acc: 0.9562    \n",
      "Epoch 6/10\n",
      "39470/39470 [==============================] - 25s - loss: 0.1042 - acc: 0.9687    \n",
      "Epoch 7/10\n",
      "39470/39470 [==============================] - 25s - loss: 0.0894 - acc: 0.9728    \n",
      "Epoch 8/10\n",
      "39470/39470 [==============================] - 25s - loss: 0.0742 - acc: 0.9772    \n",
      "Epoch 9/10\n",
      "39470/39470 [==============================] - 24s - loss: 0.0611 - acc: 0.9819    \n",
      "Epoch 10/10\n",
      "39470/39470 [==============================] - 24s - loss: 0.0505 - acc: 0.9855    \n",
      "nine_layer\n",
      "Epoch 1/10\n",
      "39470/39470 [==============================] - 26s - loss: 3.6824 - acc: 0.1901    \n",
      "Epoch 2/10\n",
      "39470/39470 [==============================] - 26s - loss: 3.2451 - acc: 0.2617    \n",
      "Epoch 3/10\n",
      "39470/39470 [==============================] - 26s - loss: 2.8560 - acc: 0.3301    \n",
      "Epoch 4/10\n",
      "39470/39470 [==============================] - 27s - loss: 2.4038 - acc: 0.4083    \n",
      "Epoch 5/10\n",
      "39470/39470 [==============================] - 27s - loss: 1.9286 - acc: 0.5052    \n",
      "Epoch 6/10\n",
      "39470/39470 [==============================] - 28s - loss: 1.4846 - acc: 0.6049    \n",
      "Epoch 7/10\n",
      "39470/39470 [==============================] - 29s - loss: 1.1324 - acc: 0.6879    \n",
      "Epoch 8/10\n",
      "39470/39470 [==============================] - 28s - loss: 0.8746 - acc: 0.7535    \n",
      "Epoch 9/10\n",
      "39470/39470 [==============================] - 28s - loss: 0.6846 - acc: 0.8040    \n",
      "Epoch 10/10\n",
      "39470/39470 [==============================] - 27s - loss: 0.5540 - acc: 0.8399    \n",
      "**************** Batch starting at 400 ********************\n",
      "shallow\n",
      "Epoch 1/10\n",
      "19650/19650 [==============================] - 9s - loss: 4.4434 - acc: 0.1217     \n",
      "Epoch 2/10\n",
      "19650/19650 [==============================] - 9s - loss: 2.4781 - acc: 0.4404     \n",
      "Epoch 3/10\n",
      "19650/19650 [==============================] - 9s - loss: 0.8790 - acc: 0.8089     \n",
      "Epoch 4/10\n",
      "19650/19650 [==============================] - 8s - loss: 0.2060 - acc: 0.9665     \n",
      "Epoch 5/10\n",
      "19650/19650 [==============================] - 9s - loss: 0.0610 - acc: 0.9922     \n",
      "Epoch 6/10\n",
      "19650/19650 [==============================] - 8s - loss: 0.0256 - acc: 0.9978     \n",
      "Epoch 7/10\n",
      "19650/19650 [==============================] - 8s - loss: 0.0166 - acc: 0.9992     \n",
      "Epoch 8/10\n",
      "19650/19650 [==============================] - 8s - loss: 0.0121 - acc: 0.9995     \n",
      "Epoch 9/10\n",
      "19650/19650 [==============================] - 8s - loss: 0.0105 - acc: 0.9996     \n",
      "Epoch 10/10\n",
      "19650/19650 [==============================] - 8s - loss: 0.0085 - acc: 0.9997     \n",
      "six_layer\n",
      "Epoch 1/10\n",
      "19650/19650 [==============================] - 12s - loss: 3.9701 - acc: 0.1829    \n",
      "Epoch 2/10\n",
      "19650/19650 [==============================] - 12s - loss: 2.2986 - acc: 0.4586    \n",
      "Epoch 3/10\n",
      "19650/19650 [==============================] - 13s - loss: 0.5452 - acc: 0.8612    \n",
      "Epoch 4/10\n",
      "19650/19650 [==============================] - 13s - loss: 0.0750 - acc: 0.9803    \n",
      "Epoch 5/10\n",
      "19650/19650 [==============================] - 12s - loss: 0.0375 - acc: 0.9896    \n",
      "Epoch 6/10\n",
      "19650/19650 [==============================] - 12s - loss: 0.0278 - acc: 0.9925    \n",
      "Epoch 7/10\n",
      "19650/19650 [==============================] - 13s - loss: 0.1666 - acc: 0.9624    \n",
      "Epoch 8/10\n",
      "19650/19650 [==============================] - 13s - loss: 0.0349 - acc: 0.9902    \n",
      "Epoch 9/10\n",
      "19650/19650 [==============================] - 14s - loss: 0.0223 - acc: 0.9940    \n",
      "Epoch 10/10\n",
      "19650/19650 [==============================] - 14s - loss: 0.0180 - acc: 0.9957    \n",
      "nine_layer\n",
      "Epoch 1/10\n",
      "19650/19650 [==============================] - 14s - loss: 3.7982 - acc: 0.1901    \n",
      "Epoch 2/10\n",
      "19650/19650 [==============================] - 14s - loss: 2.9767 - acc: 0.3211    \n",
      "Epoch 3/10\n",
      "19650/19650 [==============================] - 14s - loss: 2.2361 - acc: 0.4578    \n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19650/19650 [==============================] - 14s - loss: 1.4180 - acc: 0.6333    \n",
      "Epoch 5/10\n",
      "19650/19650 [==============================] - 14s - loss: 0.8542 - acc: 0.7617    \n",
      "Epoch 6/10\n",
      "19650/19650 [==============================] - 14s - loss: 0.5221 - acc: 0.8493    \n",
      "Epoch 7/10\n",
      "19650/19650 [==============================] - 14s - loss: 0.3888 - acc: 0.8836    \n",
      "Epoch 8/10\n",
      "19650/19650 [==============================] - 14s - loss: 0.3213 - acc: 0.9042    \n",
      "Epoch 9/10\n",
      "19650/19650 [==============================] - 14s - loss: 0.2616 - acc: 0.9233    \n",
      "Epoch 10/10\n",
      "19650/19650 [==============================] - 14s - loss: 0.2334 - acc: 0.9305    \n",
      "9760/9832 [============================>.] - ETA: 0s\n",
      "loss:6.12941059395\n",
      "acc:0.1655817738\n",
      "\n",
      "9832/9832 [==============================] - 2s     \n",
      "\n",
      "loss:7.30971293104\n",
      "acc:0.20260374288\n",
      "\n",
      "9696/9832 [============================>.] - ETA: 0s\n",
      "loss:5.95838514472\n",
      "acc:0.202705451587\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_le = None\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "for start_idx in [0,200,400]:\n",
    "    print \"**************** Batch starting at\", str(start_idx), \"********************\"\n",
    "    try:\n",
    "        del x_train\n",
    "        del y_train\n",
    "        del training_images\n",
    "        del training_labels\n",
    "        del training_files\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    x_train, training_labels, training_files = load_training_images(start_idx, batch_size=200)\n",
    "    \n",
    "    if train_le == None:\n",
    "        train_le = preprocessing.LabelEncoder() # Fit label encoder\n",
    "        train_le.fit(training_labels)\n",
    "\n",
    "    y_train = get_one_hot(train_le, training_labels)\n",
    "\n",
    "    print \"shallow\"\n",
    "    shallow.fit(x_train, y_train, batch_size=64, epochs=NUM_EPOCHS)\n",
    "    \n",
    "    print \"six_layer\"\n",
    "    six_layer.fit(x_train, y_train, batch_size=64, epochs=NUM_EPOCHS)\n",
    "    \n",
    "    print \"nine_layer\"\n",
    "    nine_layer.fit(x_train, y_train, batch_size=64, epochs=NUM_EPOCHS)\n",
    "    \n",
    "x_val, val_labels = load_validation_images()\n",
    "y_val = get_one_hot(train_le, val_labels)\n",
    "\n",
    "for model in [shallow, six_layer, nine_layer]:\n",
    "    score = model.evaluate(x_val, y_val, batch_size=32)\n",
    "    print\"\"\n",
    "    for x in range(0, len(model.metrics_names)):\n",
    "        print str(model.metrics_names[x]) + \":\" + str(score[x])\n",
    "    print \"\"\n",
    "\n",
    "shallow.save_weights(\"shallow_weights\")\n",
    "six_layer.save_weights(\"six_weights\")\n",
    "nine_layer.save_weights(\"nine_weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = shallow.evaluate(x_val, y_val, batch_size=32)\n",
    "print\"\"\n",
    "for x in range(0, len(shallow.metrics_names)):\n",
    "    print str(shallow.metrics_names[x]) + \":\" + str(score[x])\n",
    "print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### First Batch of 200\n",
    "training_images, training_labels, training_files = load_training_images(start=0, batch_size=200)\n",
    "#x_train, training_labels, training_files = randomize(training_images, training_labels, training_files)\n",
    "x_train = training_images\n",
    "train_le = preprocessing.LabelEncoder() # Fit label encoder\n",
    "train_le.fit(training_labels)\n",
    "\n",
    "y_train = get_one_hot(train_le, training_labels)\n",
    "\n",
    "shallow.fit(x_train, y_train, batch_size=64, epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model.fit(x_train, y_train, batch_size=32, epochs=1)\n",
    "######### 2nd Batch of 200\n",
    "try:\n",
    "    del x_train\n",
    "    del y_train\n",
    "    del training_images\n",
    "    del training_labels\n",
    "    del training_files\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "try:\n",
    "    print x_train\n",
    "    print training_images\n",
    "except:\n",
    "    print \"x_train and training_images not found!\"\n",
    "\n",
    "training_images, training_labels, training_files = load_training_images(start=200, batch_size=200)\n",
    "print \"after loading\"\n",
    "#x_train, training_labels, training_files = randomize(training_images, training_labels, training_files)\n",
    "x_train = training_images\n",
    "print \"after randomize\"\n",
    "y_train = get_one_hot(train_le, training_labels)\n",
    "print \"after one_hot\"\n",
    "\n",
    "shallow.fit(x_train, y_train, batch_size=64, epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "######## 3rd Batch of 100\n",
    "try:\n",
    "    del x_train\n",
    "    del y_train\n",
    "    del training_images\n",
    "    del training_labels\n",
    "    del training_files\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "try:\n",
    "    print x_train\n",
    "    print training_images\n",
    "except:\n",
    "    print \"x_train and training_images not found!\"\n",
    "\n",
    "training_images, training_labels, training_files = load_training_images(start=400, batch_size=200)\n",
    "#x_train, training_labels, training_files = randomize(training_images, training_labels, training_files)\n",
    "x_train = training_images\n",
    "y_train = get_one_hot(train_le, training_labels)\n",
    "\n",
    "shallow.fit(x_train, y_train, batch_size=64, epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val, val_labels = load_validation_images()\n",
    "y_val = get_one_hot(train_le, val_labels)\n",
    "\n",
    "score = model.evaluate(x_val, y_val, batch_size=32)\n",
    "print\"\"\n",
    "for x in range(0, len(model.metrics_names)):\n",
    "    print str(model.metrics_names[x]) + \":\" + str(score[x])\n",
    "print \"\"\n",
    "\n",
    "score = model.evaluate(x_train, y_train, batch_size=32)\n",
    "print \"\"\n",
    "\n",
    "for x in range(0, len(model.metrics_names)):\n",
    "    print str(model.metrics_names[x]) + \":\" + str(score[x])\n",
    "    \n",
    "# without dropout: acc:0.177"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
