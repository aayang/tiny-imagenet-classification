{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import os\n",
    "import matplotlib.image as mpimg\n",
    "import cv2\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "TRAIN_PATH = \"/datasets/tmp/aayang/tiny-imagenet-200/train/\"\n",
    "VAL_IMAGES_PATH = \"/datasets/tmp/aayang/tiny-imagenet-200/val/images/\"\n",
    "VAL_ANNO_FILE = \"/datasets/tmp/aayang/tiny-imagenet-200/val/val_annotations.txt\"\n",
    "TRAINING_IMAGE_SIZE = 64\n",
    "\n",
    "LOAD_BATCH_SIZE = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_images(start=0, batch_size=200):\n",
    "    images = [] #np.ndarray(shape=(batch_size*200, 64,64,3))\n",
    "    labels = []\n",
    "    file_names = []\n",
    "    class_num = 0\n",
    "    \n",
    "    for class_folder in os.listdir(TRAIN_PATH):\n",
    "        images_folder = TRAIN_PATH + class_folder + \"/noise/\"\n",
    "        x = 0\n",
    "        \n",
    "        for image_name in os.listdir(images_folder):\n",
    "            if x < start:\n",
    "                x += 1\n",
    "                continue\n",
    "            \n",
    "            image_path = os.path.join(images_folder, image_name)\n",
    "            image = mpimg.imread(image_path)\n",
    "            \n",
    "            if image.shape == (TRAINING_IMAGE_SIZE, TRAINING_IMAGE_SIZE, 3):\n",
    "                images.append(image/256.0)\n",
    "                labels.append(class_folder)\n",
    "                file_names.append(image_name)\n",
    "    \n",
    "            x += 1      \n",
    "            #if x%100 == 0: \n",
    "            #    print \"class\", class_num, \"-\", x\n",
    "                \n",
    "            if x > start+batch_size:\n",
    "                break\n",
    "        class_num += 1\n",
    "                        \n",
    "    return (np.array(images), np.array(labels), np.array(file_names))\n",
    "\n",
    "def get_one_hot(le, labels):\n",
    "    labels_encoded = le.transform(labels)\n",
    "    y = keras.utils.to_categorical(labels_encoded)\n",
    "    return y\n",
    "    \n",
    "def randomize(images, labels, files):\n",
    "    shuffle_index = np.random.permutation(len(images))\n",
    "    \n",
    "    shuffled_images = images[shuffle_index]\n",
    "    shuffled_labels = labels[shuffle_index]\n",
    "    shuffled_files  = files[shuffle_index]\n",
    "    \n",
    "    return (shuffled_images, shuffled_labels, shuffled_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_validation_images():\n",
    "    with open(VAL_ANNO_FILE) as f:\n",
    "        lines = f.read().splitlines()\n",
    "        \n",
    "    labels = []\n",
    "    images = []\n",
    "    idx = 0\n",
    "    \n",
    "    for line in lines:\n",
    "        image_att = line.split(\"\\t\")\n",
    "        image_name = image_att[0]\n",
    "        image_label = image_att[1]\n",
    "        \n",
    "        image_path = os.path.join(VAL_IMAGES_PATH, image_name)\n",
    "        image = mpimg.imread(image_path)\n",
    "        \n",
    "        if image.shape == (TRAINING_IMAGE_SIZE, TRAINING_IMAGE_SIZE, 3):\n",
    "            images.append(image/256.0)\n",
    "            labels.append(image_label)\n",
    "            \n",
    "    return (np.array(images), np.array(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Layer CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "shallow = Sequential()\n",
    "# input: 100x100 images with 3 channels -> (100, 100, 3) tensors.\n",
    "# this applies 32 convolution filters of size 3x3 each.\n",
    "shallow.add(Conv2D(64, (3, 3), activation='relu', input_shape=(64,64,3)))\n",
    "shallow.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#shallow.add(Conv2D(128, (3, 3), activation='relu', padding=\"same\"))\n",
    "#shallow.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#shallow.add(Conv2D(256, (5, 5), activation='relu', padding=\"same\"))\n",
    "#shallow.add(Conv2D(256, (5, 5), activation='relu', padding=\"same\"))\n",
    "#shallow.add(Conv2D(256, (5, 5), activation='relu', padding=\"same\"))\n",
    "#shallow.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "shallow.add(Flatten())\n",
    "shallow.add(Dense(1024, activation='relu'))\n",
    "#shallow.add(Dense(1024, activation='relu'))\n",
    "#shallow.add(Dropout(0.5))\n",
    "#shallow.add(Dense(256, activation='relu'))\n",
    "shallow.add(Dense(200, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "shallow.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=[\"accuracy\"])\n",
    "shallow.load_weights(\"shallow_weights_augment_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Layer CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "six_layer = Sequential()\n",
    "\n",
    "six_layer.add(Conv2D(64, (3, 3), activation='relu', input_shape=(64,64,3)))\n",
    "six_layer.add(Conv2D(64, (3, 3), activation='relu', padding=\"same\"))\n",
    "six_layer.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "six_layer.add(Conv2D(128, (3, 3), activation='relu', padding=\"same\"))\n",
    "six_layer.add(Conv2D(128, (3, 3), activation='relu', padding=\"same\"))\n",
    "six_layer.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "six_layer.add(Dropout(0.25))\n",
    "\n",
    "#six_layer.add(Conv2D(256, (5, 5), activation='relu', padding=\"same\"))\n",
    "#six_layer.add(Conv2D(256, (5, 5), activation='relu', padding=\"same\"))\n",
    "#six_layer.add(Conv2D(256, (5, 5), activation='relu', padding=\"same\"))\n",
    "#six_layer.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "six_layer.add(Flatten())\n",
    "six_layer.add(Dense(1024, activation='relu'))\n",
    "#six_layer.add(Dense(1024, activation='relu'))\n",
    "#six_layer.add(Dropout(0.5))\n",
    "#six_layer.add(Dense(256, activation='relu'))\n",
    "six_layer.add(Dense(200, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "six_layer.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=[\"accuracy\"])\n",
    "six_layer.load_weights(\"six_weights_augment_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9 Layer CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nine_layer = Sequential()\n",
    "# input: 100x100 images with 3 channels -> (100, 100, 3) tensors.\n",
    "# this applies 32 convolution filters of size 3x3 each.\n",
    "nine_layer.add(Conv2D(64, (3, 3), activation='relu', input_shape=(64,64,3)))\n",
    "nine_layer.add(Conv2D(64, (3, 3), activation='relu', padding=\"same\"))\n",
    "nine_layer.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "nine_layer.add(Conv2D(128, (3, 3), activation='relu', padding=\"same\"))\n",
    "nine_layer.add(Conv2D(128, (3, 3), activation='relu', padding=\"same\"))\n",
    "nine_layer.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "nine_layer.add(Dropout(0.25))\n",
    "\n",
    "nine_layer.add(Conv2D(256, (3, 3), activation='relu', padding=\"same\"))\n",
    "nine_layer.add(Conv2D(256, (3, 3), activation='relu', padding=\"same\"))\n",
    "nine_layer.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "nine_layer.add(Flatten())\n",
    "nine_layer.add(Dense(1024, activation='relu'))\n",
    "nine_layer.add(Dense(1024, activation='relu'))\n",
    "nine_layer.add(Dropout(0.5))\n",
    "nine_layer.add(Dense(200, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "nine_layer.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=[\"accuracy\"])\n",
    "nine_layer.load_weights(\"nine_weights_augment_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************** Batch starting at 0 ********************\n",
      "shallow\n",
      "Epoch 1/10\n",
      "40200/40200 [==============================] - 21s - loss: 3.6356 - acc: 0.2391    \n",
      "Epoch 2/10\n",
      "40200/40200 [==============================] - 18s - loss: 1.1824 - acc: 0.7306    \n",
      "Epoch 3/10\n",
      "40200/40200 [==============================] - 18s - loss: 0.2633 - acc: 0.9429    \n",
      "Epoch 4/10\n",
      "40200/40200 [==============================] - 18s - loss: 0.0545 - acc: 0.9903    \n",
      "Epoch 5/10\n",
      "40200/40200 [==============================] - 18s - loss: 0.0114 - acc: 0.9989    \n",
      "Epoch 6/10\n",
      "40200/40200 [==============================] - 18s - loss: 0.0042 - acc: 0.9999    \n",
      "Epoch 7/10\n",
      "40200/40200 [==============================] - 18s - loss: 0.0029 - acc: 1.0000    \n",
      "Epoch 8/10\n",
      "40200/40200 [==============================] - 18s - loss: 0.0024 - acc: 1.0000    \n",
      "Epoch 9/10\n",
      "40200/40200 [==============================] - 18s - loss: 0.0022 - acc: 1.0000    \n",
      "Epoch 10/10\n",
      "40200/40200 [==============================] - 18s - loss: 0.0020 - acc: 1.0000    \n",
      "six_layer\n",
      "Epoch 1/10\n",
      "40200/40200 [==============================] - 26s - loss: 2.0387 - acc: 0.5058    \n",
      "Epoch 2/10\n",
      "40200/40200 [==============================] - 25s - loss: 0.3660 - acc: 0.8993    \n",
      "Epoch 3/10\n",
      "40200/40200 [==============================] - 25s - loss: 0.1098 - acc: 0.9685    \n",
      "Epoch 4/10\n",
      "40200/40200 [==============================] - 26s - loss: 0.0728 - acc: 0.9780    \n",
      "Epoch 5/10\n",
      "40200/40200 [==============================] - 25s - loss: 0.0628 - acc: 0.9810    \n",
      "Epoch 6/10\n",
      "40200/40200 [==============================] - 25s - loss: 0.0500 - acc: 0.9843    \n",
      "Epoch 7/10\n",
      "40200/40200 [==============================] - 26s - loss: 0.0402 - acc: 0.9882    \n",
      "Epoch 8/10\n",
      "40200/40200 [==============================] - 26s - loss: 0.0341 - acc: 0.9897    \n",
      "Epoch 9/10\n",
      "40200/40200 [==============================] - 25s - loss: 0.0336 - acc: 0.9910    \n",
      "Epoch 10/10\n",
      "40200/40200 [==============================] - 25s - loss: 0.0299 - acc: 0.9905    \n",
      "nine_layer\n",
      "Epoch 1/10\n",
      "40200/40200 [==============================] - 28s - loss: 3.1379 - acc: 0.3075    \n",
      "Epoch 2/10\n",
      "40200/40200 [==============================] - 28s - loss: 2.1108 - acc: 0.4914    \n",
      "Epoch 3/10\n",
      "40200/40200 [==============================] - 28s - loss: 1.3949 - acc: 0.6424    \n",
      "Epoch 4/10\n",
      "40200/40200 [==============================] - 28s - loss: 0.9109 - acc: 0.7555    \n",
      "Epoch 5/10\n",
      "40200/40200 [==============================] - 27s - loss: 0.6580 - acc: 0.8185    \n",
      "Epoch 6/10\n",
      "40200/40200 [==============================] - 28s - loss: 0.5254 - acc: 0.8504    \n",
      "Epoch 7/10\n",
      "40200/40200 [==============================] - 28s - loss: 0.4451 - acc: 0.8732    \n",
      "Epoch 8/10\n",
      "40200/40200 [==============================] - 27s - loss: 0.3607 - acc: 0.8954    \n",
      "Epoch 9/10\n",
      "40200/40200 [==============================] - 27s - loss: 0.3314 - acc: 0.9064    \n",
      "Epoch 10/10\n",
      "40200/40200 [==============================] - 28s - loss: 0.2890 - acc: 0.9193    \n",
      "**************** Batch starting at 200 ********************\n",
      "shallow\n",
      "Epoch 1/10\n",
      "40200/40200 [==============================] - 17s - loss: 3.5678 - acc: 0.2455    \n",
      "Epoch 2/10\n",
      "40200/40200 [==============================] - 17s - loss: 1.0254 - acc: 0.7628    \n",
      "Epoch 3/10\n",
      "40200/40200 [==============================] - 17s - loss: 0.2127 - acc: 0.9546    \n",
      "Epoch 4/10\n",
      "40200/40200 [==============================] - 17s - loss: 0.0411 - acc: 0.9931    \n",
      "Epoch 5/10\n",
      "40200/40200 [==============================] - 17s - loss: 0.0099 - acc: 0.9991    \n",
      "Epoch 6/10\n",
      "40200/40200 [==============================] - 17s - loss: 0.0041 - acc: 1.0000    \n",
      "Epoch 7/10\n",
      "40200/40200 [==============================] - 17s - loss: 0.0030 - acc: 1.0000    \n",
      "Epoch 8/10\n",
      "40200/40200 [==============================] - 17s - loss: 0.0026 - acc: 1.0000    \n",
      "Epoch 9/10\n",
      "40200/40200 [==============================] - 17s - loss: 0.0023 - acc: 1.0000    \n",
      "Epoch 10/10\n",
      "40200/40200 [==============================] - 17s - loss: 0.0021 - acc: 1.0000    \n",
      "six_layer\n",
      "Epoch 1/10\n",
      "40200/40200 [==============================] - 25s - loss: 1.9652 - acc: 0.5161    \n",
      "Epoch 2/10\n",
      "40200/40200 [==============================] - 25s - loss: 0.3185 - acc: 0.9089    \n",
      "Epoch 3/10\n",
      "40200/40200 [==============================] - 25s - loss: 0.0982 - acc: 0.9701    \n",
      "Epoch 4/10\n",
      "40200/40200 [==============================] - 25s - loss: 0.0690 - acc: 0.9782    \n",
      "Epoch 5/10\n",
      "40200/40200 [==============================] - 25s - loss: 0.0532 - acc: 0.9834    \n",
      "Epoch 6/10\n",
      "40200/40200 [==============================] - 25s - loss: 0.0432 - acc: 0.9865    \n",
      "Epoch 7/10\n",
      "40200/40200 [==============================] - 25s - loss: 0.0378 - acc: 0.9887    \n",
      "Epoch 8/10\n",
      "40200/40200 [==============================] - 25s - loss: 0.0333 - acc: 0.9897    \n",
      "Epoch 9/10\n",
      "40200/40200 [==============================] - 25s - loss: 0.0277 - acc: 0.9908    \n",
      "Epoch 10/10\n",
      "40200/40200 [==============================] - 25s - loss: 0.0353 - acc: 0.9890    \n",
      "nine_layer\n",
      "Epoch 1/10\n",
      "40200/40200 [==============================] - 27s - loss: 3.2014 - acc: 0.2941    \n",
      "Epoch 2/10\n",
      "40200/40200 [==============================] - 27s - loss: 2.2707 - acc: 0.4576    \n",
      "Epoch 3/10\n",
      "40200/40200 [==============================] - 28s - loss: 1.5720 - acc: 0.6018    \n",
      "Epoch 4/10\n",
      "40200/40200 [==============================] - 28s - loss: 1.0349 - acc: 0.7260    \n",
      "Epoch 5/10\n",
      "40200/40200 [==============================] - 28s - loss: 0.7294 - acc: 0.7951    \n",
      "Epoch 6/10\n",
      "40200/40200 [==============================] - 28s - loss: 0.5356 - acc: 0.8481    \n",
      "Epoch 7/10\n",
      "40200/40200 [==============================] - 28s - loss: 0.4397 - acc: 0.8738    \n",
      "Epoch 8/10\n",
      "40200/40200 [==============================] - 28s - loss: 0.3574 - acc: 0.8956    \n",
      "Epoch 9/10\n",
      "40200/40200 [==============================] - 28s - loss: 0.3610 - acc: 0.8963    \n",
      "Epoch 10/10\n",
      "40200/40200 [==============================] - 28s - loss: 0.3286 - acc: 0.9060    \n",
      "**************** Batch starting at 400 ********************\n",
      "shallow\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 9s - loss: 3.6339 - acc: 0.2392     \n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 9s - loss: 0.8750 - acc: 0.8010     \n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 10s - loss: 0.1640 - acc: 0.9668    \n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 10s - loss: 0.0391 - acc: 0.9945    \n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 10s - loss: 0.0141 - acc: 0.9988    \n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 9s - loss: 0.0086 - acc: 0.9996     \n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 9s - loss: 0.0070 - acc: 0.9996     \n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 9s - loss: 0.0061 - acc: 0.9997     \n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 9s - loss: 0.0056 - acc: 0.9997     \n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 9s - loss: 0.0052 - acc: 0.9998     \n",
      "six_layer\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 13s - loss: 2.1704 - acc: 0.4783    \n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 13s - loss: 0.2954 - acc: 0.9162    \n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 12s - loss: 0.0661 - acc: 0.9809    \n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 13s - loss: 0.0377 - acc: 0.9888    \n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 13s - loss: 0.0214 - acc: 0.9941    \n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 13s - loss: 0.0243 - acc: 0.9931    \n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 13s - loss: 0.0159 - acc: 0.9954    \n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 13s - loss: 0.0186 - acc: 0.9944    \n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 13s - loss: 0.0166 - acc: 0.9953    \n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 13s - loss: 0.0110 - acc: 0.9970    \n",
      "nine_layer\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 14s - loss: 3.5049 - acc: 0.2563    \n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 14s - loss: 2.4937 - acc: 0.4203    \n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 14s - loss: 1.6782 - acc: 0.5793    \n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 14s - loss: 1.0314 - acc: 0.7233    \n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 14s - loss: 0.6195 - acc: 0.8253    \n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 14s - loss: 0.4476 - acc: 0.8691    \n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 14s - loss: 0.3226 - acc: 0.9063    \n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 14s - loss: 0.2576 - acc: 0.9239    \n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 14s - loss: 0.2208 - acc: 0.9354    \n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 14s - loss: 0.2137 - acc: 0.9380    \n",
      "9760/9832 [============================>.] - ETA: 0s\n",
      "loss:9.47928264453\n",
      "acc:0.129170056957\n",
      "\n",
      "9824/9832 [============================>.] - ETA: 0s\n",
      "loss:9.12857957658\n",
      "acc:0.175956061839\n",
      "\n",
      "9824/9832 [============================>.] - ETA: 0s\n",
      "loss:7.73758373152\n",
      "acc:0.19253458096\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_le = None\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "for start_idx in [0,200,400]:\n",
    "    print \"**************** Batch starting at\", str(start_idx), \"********************\"\n",
    "\n",
    "    try:\n",
    "        del x_train\n",
    "        del y_train\n",
    "        del training_images\n",
    "        del training_labels\n",
    "        del training_files\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    x_train, training_labels, training_files = load_training_images(start_idx, batch_size=200)\n",
    "    \n",
    "    if train_le == None:\n",
    "        train_le = preprocessing.LabelEncoder() # Fit label encoder\n",
    "        train_le.fit(training_labels)\n",
    "\n",
    "    y_train = get_one_hot(train_le, training_labels)\n",
    "\n",
    "    print \"shallow\"\n",
    "    shallow.fit(x_train, y_train, batch_size=64, epochs=NUM_EPOCHS)\n",
    "    \n",
    "    print \"six_layer\"\n",
    "    six_layer.fit(x_train, y_train, batch_size=64, epochs=NUM_EPOCHS)\n",
    "    \n",
    "    print \"nine_layer\"\n",
    "    nine_layer.fit(x_train, y_train, batch_size=64, epochs=NUM_EPOCHS)\n",
    "\n",
    "x_val, val_labels = load_validation_images()\n",
    "y_val = get_one_hot(train_le, val_labels)\n",
    "\n",
    "for model in [shallow, six_layer, nine_layer]:\n",
    "    score = model.evaluate(x_val, y_val, batch_size=32)\n",
    "    print\"\"\n",
    "    for x in range(0, len(model.metrics_names)):\n",
    "        print str(model.metrics_names[x]) + \":\" + str(score[x])\n",
    "    print \"\"\n",
    "\n",
    "shallow.save_weights(\"shallow_weights_augment_4\")\n",
    "six_layer.save_weights(\"six_weights_augment_4\")\n",
    "nine_layer.save_weights(\"nine_weights_augment_4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### First Batch of 200\n",
    "training_images, training_labels, training_files = load_training_images(start=0, batch_size=200)\n",
    "#x_train, training_labels, training_files = randomize(training_images, training_labels, training_files)\n",
    "x_train = training_images\n",
    "train_le = preprocessing.LabelEncoder() # Fit label encoder\n",
    "train_le.fit(training_labels)\n",
    "\n",
    "y_train = get_one_hot(train_le, training_labels)\n",
    "\n",
    "shallow.fit(x_train, y_train, batch_size=64, epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model.fit(x_train, y_train, batch_size=32, epochs=1)\n",
    "######### 2nd Batch of 200\n",
    "try:\n",
    "    del x_train\n",
    "    del y_train\n",
    "    del training_images\n",
    "    del training_labels\n",
    "    del training_files\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "try:\n",
    "    print x_train\n",
    "    print training_images\n",
    "except:\n",
    "    print \"x_train and training_images not found!\"\n",
    "\n",
    "training_images, training_labels, training_files = load_training_images(start=200, batch_size=200)\n",
    "print \"after loading\"\n",
    "#x_train, training_labels, training_files = randomize(training_images, training_labels, training_files)\n",
    "x_train = training_images\n",
    "print \"after randomize\"\n",
    "y_train = get_one_hot(train_le, training_labels)\n",
    "print \"after one_hot\"\n",
    "\n",
    "shallow.fit(x_train, y_train, batch_size=64, epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "######## 3rd Batch of 100\n",
    "try:\n",
    "    del x_train\n",
    "    del y_train\n",
    "    del training_images\n",
    "    del training_labels\n",
    "    del training_files\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "try:\n",
    "    print x_train\n",
    "    print training_images\n",
    "except:\n",
    "    print \"x_train and training_images not found!\"\n",
    "\n",
    "training_images, training_labels, training_files = load_training_images(start=400, batch_size=200)\n",
    "#x_train, training_labels, training_files = randomize(training_images, training_labels, training_files)\n",
    "x_train = training_images\n",
    "y_train = get_one_hot(train_le, training_labels)\n",
    "\n",
    "shallow.fit(x_train, y_train, batch_size=64, epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val, val_labels = load_validation_images()\n",
    "y_val = get_one_hot(train_le, val_labels)\n",
    "\n",
    "score = model.evaluate(x_val, y_val, batch_size=32)\n",
    "print\"\"\n",
    "for x in range(0, len(model.metrics_names)):\n",
    "    print str(model.metrics_names[x]) + \":\" + str(score[x])\n",
    "print \"\"\n",
    "\n",
    "score = model.evaluate(x_train, y_train, batch_size=32)\n",
    "print \"\"\n",
    "\n",
    "for x in range(0, len(model.metrics_names)):\n",
    "    print str(model.metrics_names[x]) + \":\" + str(score[x])\n",
    "    \n",
    "# without dropout: acc:0.177"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
